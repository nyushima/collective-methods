###### Gathers the headlines from news sights. I had success the first time I ran it, but in future runs, the code seemed to be broken 
###### it says "links" is not defined
<pre>
url="http://www.aljazeera.com"
page = urllib.request.urlopen(url)
soup = BeautifulSoup(page.read(), 'html.parser')
header = soup.find_all('h1')
linksJson = []

for link in header:
    linkText = link.text
    linkHref = link.get('text')
    linksJson.append({'text':linkText, 'href': linkHref})

with open('soup.json', 'w') as jsonFile:
    json.dump(linksJson, jsonFile)
</pre>


###### Grabs and prettifies links from reddit. It works but gets a "too many requests" error after a point.
<pre>
from bs4 import BeautifulSoup
import urllib.request
import json

url = 'http://www.reddit.com'
page = urllib.request.urlopen(url)
soup = BeautifulSoup(page.read(), 'html.parser')
links = soup.find_all('a')
linksJson = []

for link in links:
    linkText = link.text
    linkHref = link.get('href')
    linksJson.append({'text':linkText, 'href': linkHref})

print (soup.prettify())
</pre>

######An Attempt to scrape images from a website dedicated to beautiful soup
<pre>
from bs4 import BeautifulSoup
import urllib.request
import csv

url = 'http://www.gregreda.com/2013/03/03/web-scraping-101-with-python/'
page = urllib.request.urlopen(url)
soup = BeautifulSoup(page.read())
image = soup.find_all('img')
csvFile = csv.writer(open('soup.csv', 'w'))
csvFile.writerow(['text','href'])

for pix in image:
   	pixHref = pix.get('href')
    csvFile.writerow([pixHref])
</pre>

