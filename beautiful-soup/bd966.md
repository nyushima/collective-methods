# Bingling's experiments with Beautiful Soup
----
###### Getting title and link of Youtube videos on one channel.

````python
from bs4 import BeautifulSoup
import urllib.request
import csv

url = 'https://www.youtube.com/user/anflowny/videos'
#The url can be changed into any other channels.
page = urllib.request.urlopen(url)
soup = BeautifulSoup(page.read())
links = soup.find_all('a')
csvFile = csv.writer(open('soup1.csv', 'w'))
csvFile.writerow(['text','href'])

for link in links:
    linkText = link.text
    linkHref = link.get('href')
    csvFile.writerow([linkText,linkHref])
````
Kept receiving alerts like: TypeError: POST data should be bytes or an iterable of bytes. It cannot be str  
Tried to add duration using .next_siblings and .descendants, but both methods failed.


###### Getting title, link, description of Baidu search result. (Still in progress)

````python
from bs4 import BeautifulSoup
import urllib.request

url = 'http://www.baidu.com/s?wd=python&rsv_spt=1&rsv_iqid=0xcb5b2d000005d26a&issp=1&f=8&rsv_bp=0&rsv_idx=2&ie=utf-8&tn=baiduhome_pg&rsv_enter=1&rsv_sug3=5&rsv_sug2=0&inputT=935&rsv_sug4=936'
page = urllib.request.urlopen(url)
soup = BeautifulSoup(page.read(), "html.parser")

Rlist = soup.find_all("div", class_="result c-container ")

print(Rlist)
````
Still working on getting more accurate results. Now it can print out the parent sibling of each search result's url, title and all detailed information.  
However, it would take me a bit more time to figure out the way to drag these detail out from the large information pack.

###### Searching for company information in 51 job.

````python
from bs4 import BeautifulSoup
import urllib.request
import csv

url = 'http://www.51job.com/en/shanghai'
page = urllib.request.urlopen(url)
soup = BeautifulSoup(page.read(), "html.parser")
links = soup.find_all('a')
csvFile = csv.writer(open('soup3.csv', 'w'))
csvFile.writerow(['title','href','id'])

for link in links:
    linkText = link.get('title')
    linkHref = link.get('href')
    linkId = link.get('adid')
    csvFile.writerow([linkText,linkHref,linkId])
````
Failure: too much data, connection timeout. Need simplify and add function to scrape pics as well.
